In this last video, we'll take a look at how the mongoDB database is deployed.
If we look at our mongodb.yaml file, this is perhaps the most complicated one with 4 different parts. But don't worry, we'll break them down one part at a time:

1. We start with something familiar, the service component. This is very similar to the service objects you've seen in the rabbitMQ deployment, except that we're selecting pods with the label app:mongodb. This line must match the template field in our replicaSet.
1. Speaking of which, you've might have already noticed that we're using a replicaSet instead of Deployment. Our goal here is to run a simple storage solution by running a single pod that runs the database. For such singleton application, both replicaSet and deployment objects work. We can of course simply create a pod, but in the case of node failure, the Pod will not be rescheduled onto another node. It's almost always better to use high level resources, even when managing a single pod. 
1. For larger scale applications you might need to use something call statefulSets, but for many smaller scale applications, the complexity tradeoff is sometimes worth it. 
1. Everything should be similar to the deployment resource from the last video, but there are a few difference. First, we create a volume call mongodb-data by using the persistentVolumeClaim. You might not be familiar with persistentVolumeClaim but we'll go over it here in a minute. Notice this volume is created on the Pod level, meaning all containers within this Pod can mount this volume. Next, we mount the volume inside our container definition. The mount path data/db is the default dbpath mongodb.
1. Next, we'll look at the persistentVolume and the persistentVolumeClaim, both are integral part of kubernetes storage. Storage is unique within Kubernetes in the sense that we must assume the the role of developer and Kubernetes administrator. 
1. Developer needs to access resources such as CPU, memory, and storage without knowing how they're provisioned or where they are from. Developer wants to simply ask for 1 CPU or 1GB of disk space; such decoupling is one of the central ideas Kubernetes. Administrator on the other hand, needs to know how to provision resources. Up until this point, we've only worked with resources such as CPU and RAM, which are automatically provisioned by GKE. But for storage, we must do it ourselves. We'll first create a Network attach storage, then make it known to Kubernetes using PersistentVolume, and finally, our developer-self can claim the storage using PersistentVolumeClaim.
1. So first we'll run `gcloud container clusters list` to get the compute zone of our cluster. Our disk has to be in the same zone as our cluster. Once we have the zone, we'll use the command `gcloud compute disks create` follow by the size, zone, and disk name. It's going to give a warning that our disk is too small but we'll ignore that for this demo. 
1. Once the disk is created, we create a persistentVolume resource in Kubernetes, be sure to put the name of your disk, for me is mongodb, into the pdName field. It is also important to tell Kubernetes that the technology behind the disk is gcePersistentDisk, as Kubernetes supports many different types of NAS. Access mode determines if the disk can be read and written to by multiple nodes or a single node.
1. At this point, we can put our administrator hat away and become a developer again. We'll create a PersistentVolumeClaim. In the PVC, noticed there is no mention of the specific technology used by the storage. All we do is use a selector to bind it to our persistentVolume. And specify how much storage space we need.
1. This completes the setup for storage. However, keep in mind we're using the simplest setup. If you're interested, look into dynamic provisioning and stateful sets.

This is the end of this video series.